{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgGxxLQFPVzX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer,AlbertTokenizer,AutoTokenizer, AutoModelForQuestionAnswering ,BertForQuestionAnswering\n",
        "import torch\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "5JSV_oTscJzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.data.processors.squad import SquadV2Processor\n",
        "\n",
        "# this processor loads the SQuAD2.0 dev set examples\n",
        "processor = SquadV2Processor()\n",
        "examples = processor.get_dev_examples(\"./data/squad/\", filename=\"/content/dev-v2.0.json\")"
      ],
      "metadata": {
        "id": "0egId8I1wCDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_example(qid):\n",
        "    from pprint import pprint\n",
        "\n",
        "    idx = qid_to_example_index[qid]\n",
        "    q = examples[idx].question_text\n",
        "    c = examples[idx].context_text\n",
        "    a = [answer['text'] for answer in examples[idx].answers]\n",
        "\n",
        "    print(f'Example {idx} of {len(examples)}\\n---------------------')\n",
        "    print(f\"Q: {q}\\n\")\n",
        "    print(\"Context:\")\n",
        "    pprint(c)\n",
        "    print(f\"\\nTrue Answers:\\n{a}\")"
      ],
      "metadata": {
        "id": "bpCuou4kxeWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qid_to_example_index = {example.qas_id: i for i, example in enumerate(examples)}\n",
        "qid_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\n",
        "answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if has_answer]\n",
        "no_answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if not has_answer]"
      ],
      "metadata": {
        "id": "TMMFroa4xnS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    prec = len(common_tokens) / len(pred_tokens)\n",
        "    rec = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def compute_acc(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens) #TP\n",
        "    uncommon_tokens =set(pred_tokens) | set(truth_tokens) #\n",
        "\n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    prec = len(common_tokens) / len(pred_tokens) #TP+FP\n",
        "    rec = len(common_tokens) / len(truth_tokens) #TP+FL\n",
        "\n",
        "    return\n",
        "\n",
        "def get_gold_answers(example):\n",
        "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
        "\n",
        "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
        "\n",
        "    # if gold_answers doesn't exist it's because this is a negative example -\n",
        "    # the only correct answer is an empty string\n",
        "    if not gold_answers:\n",
        "        gold_answers = [\"\"]\n",
        "\n",
        "    return gold_answers"
      ],
      "metadata": {
        "id": "BwYQK2GLxyFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT"
      ],
      "metadata": {
        "id": "0hvDKcdB1m_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "V5mef4lzssZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "8pG--UAGs10g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
      ],
      "metadata": {
        "id": "IIeI-suyKdTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ],
      "metadata": {
        "id": "Ym-zrvY2tPFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the\n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for token, id in zip(tokens, input_ids):\n",
        "\n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "\n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')"
      ],
      "metadata": {
        "id": "UQpOZ9potQmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of segment A tokens includes the [SEP] token istelf.\n",
        "num_seg_a = sep_index + 1\n",
        "\n",
        "# The remainder are segment B.\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "# Construct the list of 0s and 1s.\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "metadata": {
        "id": "jfp4EaeUtURg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run our example through the model.\n",
        "outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                             token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                             return_dict=True)\n",
        "\n",
        "start_scores = outputs.start_logits\n",
        "end_scores = outputs.end_logits"
      ],
      "metadata": {
        "id": "w-_yrPtntXMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_scores)\n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "metadata": {
        "id": "TXf4WGLxtbDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with the first token.\n",
        "answer = tokens[answer_start]\n",
        "\n",
        "# Select the remaining answer tokens and join them with whitespace.\n",
        "for i in range(answer_start + 1, answer_end + 1):\n",
        "\n",
        "    # If it's a subword token, then recombine it with the previous token.\n",
        "    if tokens[i][0:2] == '##':\n",
        "        answer += tokens[i][2:]\n",
        "\n",
        "    # Otherwise, add a space then the token.\n",
        "    else:\n",
        "        answer += ' ' + tokens[i]\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "metadata": {
        "id": "_9bL9wkjtean"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "#sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (16,8)"
      ],
      "metadata": {
        "id": "Zqo70kaytg7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_scores = start_scores.detach().numpy().flatten()\n",
        "e_scores = end_scores.detach().numpy().flatten()\n",
        "\n",
        "# We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
        "# to be unique, so we'll add the token index to the end of each one.\n",
        "token_labels = []\n",
        "for (i, token) in enumerate(tokens):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, i))"
      ],
      "metadata": {
        "id": "zImTL1Frtllf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a barplot showing the start word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('Start Word Scores')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FptvJ9tBtou3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a barplot showing the end word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=e_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('End Word Scores')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YlfxdCHFtzbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Store the tokens and scores in a DataFrame.\n",
        "# Each token will have two rows, one for its start score and one for its end\n",
        "# score. The \"marker\" column will differentiate them. A little wacky, I know.\n",
        "scores = []\n",
        "for (i, token_label) in enumerate(token_labels):\n",
        "\n",
        "    # Add the token's start score as one row.\n",
        "    scores.append({'token_label': token_label,\n",
        "                   'score': s_scores[i],\n",
        "                   'marker': 'start'})\n",
        "\n",
        "    # Add  the token's end score as another row.\n",
        "    scores.append({'token_label': token_label,\n",
        "                   'score': e_scores[i],\n",
        "                   'marker': 'end'})\n",
        "\n",
        "df = pd.DataFrame(scores)"
      ],
      "metadata": {
        "id": "MmEdTHzpt3-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw a grouped barplot to show start and end scores for each word.\n",
        "# The \"hue\" parameter is where we tell it which datapoints belong to which\n",
        "# of the two series.\n",
        "g = sns.catplot(x=\"token_label\", y=\"score\", hue=\"marker\", data=df,\n",
        "                kind=\"bar\", height=6, aspect=4)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "g.set_xticklabels(g.ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "g.ax.grid(True)"
      ],
      "metadata": {
        "id": "rFGdDJN9t9H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, answer_text):\n",
        "\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    outputs = model(torch.tensor([input_ids]),\n",
        "                    token_type_ids=torch.tensor([segment_ids]),\n",
        "                    return_dict=True)\n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer = tokens[answer_start]\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "metadata": {
        "id": "PRuZka3suBhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80)"
      ],
      "metadata": {
        "id": "SeTiAXP7uHJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Please enter your text: \\n\")\n",
        "print(\"Your Text is \")\n",
        "print(wrapper.fill(text))\n",
        "question = input(\"\\nPlease enter your question: \\n\")\n",
        "while True:\n",
        "    answer_question(question, text)\n",
        "\n",
        "    flag = True\n",
        "    flag_N = False\n",
        "\n",
        "    while flag:\n",
        "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
        "        if response[0] == \"Y\":\n",
        "            question = input(\"\\nPlease enter your question: \\n\")\n",
        "            flag = False\n",
        "        elif response[0] == \"N\":\n",
        "            print(\"\\nBye!\")\n",
        "            flag = False\n",
        "            flag_N = True\n",
        "\n",
        "    if flag_N == True:\n",
        "        break"
      ],
      "metadata": {
        "id": "SkQ_ae8ZHlS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(qid,model):\n",
        "    # given a question id (qas_id or qid), load the example, get the model outputs and generate an answer\n",
        "    question = examples[qid_to_example_index[qid]].question_text\n",
        "    context = examples[qid_to_example_index[qid]].context_text\n",
        "\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(outputs[1]) + 1\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "LE7VKGi_yvXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Question_Answer(question,answer_text,model,tokenizer):\n",
        "  inputs = tokenizer.encode_plus(question, answer_text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "  input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "  print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "  text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "  print(text_tokens)\n",
        "  outputs = model(**inputs)\n",
        "  answer_start_scores=outputs.start_logits\n",
        "  answer_end_scores=outputs.end_logits\n",
        "\n",
        "  answer_start = torch.argmax(\n",
        "      answer_start_scores\n",
        "  )  # Get the most likely beginning of answer with the argmax of the score\n",
        "  answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "\n",
        "  # Combine the tokens in the answer and print it out.\"\"\n",
        "  answer = answer.replace(\"#\",\"\")\n",
        "\n",
        "  print('Answer: \"' + answer + '\"')\n",
        "  return answer"
      ],
      "metadata": {
        "id": "GO5IUyduTuGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Evaluating the BERT model"
      ],
      "metadata": {
        "id": "TnMarnFpyy6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction =get_prediction(answer_qids[1300],model)\n",
        "example = examples[qid_to_example_index[answer_qids[1300]]]\n",
        "\n",
        "gold_answers = get_gold_answers(example)\n",
        "\n",
        "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "\n",
        "print(f\"Question: {example.question_text}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"True Answers: {gold_answers}\")\n",
        "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
      ],
      "metadata": {
        "id": "0rCveIDeyS7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ALBERT: A Lite BERT"
      ],
      "metadata": {
        "id": "_o3O_1olbafP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline('question-answering', model='valhalla/electra-base-discriminator-finetuned_squadv1')"
      ],
      "metadata": {
        "id": "EmYJvuBkgoAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained('ahotrod/albert_xxlargev1_squad2_512')\n",
        "model=AutoModelForQuestionAnswering.from_pretrained('ahotrod/albert_xxlargev1_squad2_512')\n",
        "question = \"How heavy is Ever Given?\"\n",
        "answer_text = \"The Ever Given is 400m-long (1,312ft) and weighs 200,000 tonnes, with a maximum capacity of 20,000 containers. It is currently carrying 18,300 containers.\"\n",
        "Question_Answer(question,answer_text,model,tokenizer)"
      ],
      "metadata": {
        "id": "fnrvtHybbfuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Evaluating the ALBERT model"
      ],
      "metadata": {
        "id": "HbwqozQZzw2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction =get_prediction(answer_qids[1300],model)\n",
        "example = examples[qid_to_example_index[answer_qids[1300]]]\n",
        "\n",
        "gold_answers = get_gold_answers(example)\n",
        "\n",
        "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "\n",
        "print(f\"Question: {example.question_text}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"True Answers: {gold_answers}\")\n",
        "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
      ],
      "metadata": {
        "id": "aaCLyAjvyW8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)"
      ],
      "metadata": {
        "id": "D-4vXQbkdpq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/electra-base-discriminator-finetuned_squadv1\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/electra-base-discriminator-finetuned_squadv1\")\n",
        "\n",
        "question=\"What is the discriminator of ELECTRA similar to?\"\n",
        "answer_text='As mentioned in the original paper: ELECTRA is a new method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish \"real\" input tokens vs \"fake\" input tokens generated by another neural network, similar to the discriminator of a GAN. At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the SQuAD 2.0 dataset.'\n",
        "Question_Answer(question,answer_text,model,tokenizer)"
      ],
      "metadata": {
        "id": "zHD91HlEdqsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Evaluating the ELECTRA model"
      ],
      "metadata": {
        "id": "YApYw6U7z7T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction =get_prediction(answer_qids[1300],model)\n",
        "example = examples[qid_to_example_index[answer_qids[1300]]]\n",
        "\n",
        "gold_answers = get_gold_answers(example)\n",
        "\n",
        "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "\n",
        "print(f\"Question: {example.question_text}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"True Answers: {gold_answers}\")\n",
        "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
      ],
      "metadata": {
        "id": "vfUbXaLzz5Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BART"
      ],
      "metadata": {
        "id": "0IY_zu5RjCk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")\n",
        "question=\"Upto how many tokens of sequences can BART handle?\"\n",
        "answer_text=\"To use BART for question answering tasks, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word. This representation is used to classify the token. As given in the paper bart-large achives comparable to ROBERTa on SQuAD. Another notable thing about BART is that it can handle sequences with upto 1024 tokens.\"\n",
        "Question_Answer(question,answer_text,model,tokenizer)"
      ],
      "metadata": {
        "id": "y4BvXzedjKBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem arrives at Long document question-answering.The Normal question answering models only take upto maximum of 512 tokens and gives Run-time errors above the threshold. so to correct that we use Longformer which can bares upto 4096 tokens."
      ],
      "metadata": {
        "id": "k0hFQBCZltxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Evaluating the BART model"
      ],
      "metadata": {
        "id": "tKLYaCKI0B9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction =get_prediction(answer_qids[1300],model)\n",
        "example = examples[qid_to_example_index[answer_qids[1300]]]\n",
        "\n",
        "gold_answers = get_gold_answers(example)\n",
        "\n",
        "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "\n",
        "print(f\"Question: {example.question_text}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"True Answers: {gold_answers}\")\n",
        "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
      ],
      "metadata": {
        "id": "F_Pekhyo0BV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Longformer: the Long-Document Transformer"
      ],
      "metadata": {
        "id": "LcU4pwqmlofu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")"
      ],
      "metadata": {
        "id": "AO76GYcXlpi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Please enter your text: \\n\")\n",
        "print(\"Your Text is \")\n",
        "print(wrapper.fill(text))\n",
        "question = input(\"\\nPlease enter your question: \\n\")\n",
        "while True:\n",
        "    Question_Answer(question, text,model,tokenizer)\n",
        "\n",
        "    flag = True\n",
        "    flag_N = False\n",
        "\n",
        "    while flag:\n",
        "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
        "        if response[0] == \"Y\":\n",
        "            question = input(\"\\nPlease enter your question: \\n\")\n",
        "            flag = False\n",
        "        elif response[0] == \"N\":\n",
        "            print(\"\\nBye!\")\n",
        "            flag = False\n",
        "            flag_N = True\n",
        "\n",
        "    if flag_N == True:\n",
        "        break"
      ],
      "metadata": {
        "id": "TYKRZVxFVEvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Evaluating the Longformer model"
      ],
      "metadata": {
        "id": "Uf6NucZQ0Pq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction =get_prediction(answer_qids[1300],model)\n",
        "example = examples[qid_to_example_index[answer_qids[1300]]]\n",
        "\n",
        "gold_answers = get_gold_answers(example)\n",
        "\n",
        "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "\n",
        "print(f\"Question: {example.question_text}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"True Answers: {gold_answers}\")\n",
        "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
      ],
      "metadata": {
        "id": "9BcSEtkSvqAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Metrices\n",
        "\n",
        "**<h4>1. Exact Match(EM)**\n",
        "\n",
        "EM = 1 if matches otherwise EM = 0.\n",
        "\n",
        "\n",
        "**2. F1 Score= 2 * precision * recall /(precision+recall)**\n",
        "\n",
        "0<F1<1\n",
        "\n"
      ],
      "metadata": {
        "id": "OuJ_ClNg1VeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Evaluating the Above Models on SQuAD2.0 dev set"
      ],
      "metadata": {
        "id": "8r4VZjh36mGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT:\n",
        "\n",
        "                            EM : 0\n",
        "\n",
        "                            F1 : 0.8\n",
        "ALBERT:\n",
        "\n",
        "                            EM : 0\n",
        "\n",
        "                            F1: 0.8\n",
        "ELECTRA:\n",
        "\n",
        "                            EM : 0\n",
        "\n",
        "                            F1: 0.9090909090909091\n",
        "BART:\n",
        "\n",
        "                            EM : 1\n",
        "\n",
        "                            F1: 0.8333333333333334\n",
        "LONGFORMER:\n",
        "\n",
        "                            EM : 0\n",
        "\n",
        "                            F1: 0.8"
      ],
      "metadata": {
        "id": "dAFJj5kU4j3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "On conclusion, Among all the above models,\n",
        "\n",
        "ELECTRA gives highest Evaluation score (F1)\n",
        "\n",
        "and\n",
        "\n",
        "LONGFORM takes the highest number of tokens (4096).\n"
      ],
      "metadata": {
        "id": "HHXoFsf97wwb"
      }
    }
  ]
}
